---
title: 'Why Mean Squared Error?!!'
date: 2018/10/30
permalink: /posts/2012/08/blog-post-4/
tags:
  - SML
  - Mean Squared Error
  - Normal Distribution
---

# Why Mean Squared Error? 
For me, a question arises when people use **MSE** for their learning tasks. The question is: **WHY??** But if you ask this question you probably get answers such as: 
1. since it works well on this dataset 
2. since there we want to give more penalty for bad predictions 
3. computating the derivation of MSE is simple
These answers do not convince me. For the first answer we can ask ok it works well but maybe there is some better choices. For the second and third answers we can say, there is other alternative for giving penalty for bad predictions with easy computations.<br>
But I found one reason (for particular situation) in Wasserman's "All of statistics" that make me a little calmer! For other students not interested in the above reasons, I want to share this post to hopefully make them calmer. 
The book states that in linear regression with normal noise if we want to use Maximum Likelihood to learn parameter, it is the same as minimizing the MSE. <br>
First, let's define linear regression.<br>
## Linear Regression
Suppose for each data, we have n-dimensional vector X and label Y. We assume that there is a linear relationship between X and y. (ie a.X+b=Y where a )


Headings are cool
======

You can have many headings
======

Aren't headings cool?
------
