{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Mean Squared Error? \n",
    "For me, a question arises when people use **MSE** for their learning tasks. The question is: **WHY??** But if you ask this question you probably get answers such as: \n",
    "1. Since it works well on this dataset \n",
    "2. Because there we want to give more penalty for bad predictions \n",
    "3. Computating the derivation of MSE is simple <br>\n",
    "\n",
    "These answers do not convince me. For the first answer we can ask: \"ok it works well but maybe there is some better choices.\" For the second and third answers we can say: \"there is other alternative for giving penalty for bad predictions with easy computations. (like Error to the power 4)\"<br> <br>\n",
    "But fortunately I found one reason (for particular situation) in Wasserman's \"All of statistics\" that make me a little calmer! For other students not interested in the above reasons, I want to share this post to hopefully make them calmer. <br>\n",
    "The book states that in linear regression with normal noise if we want to use Maximum Likelihood to learn parameter, it is the same as minimizing the MSE. <br>\n",
    "First, let's define linear regression.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of Random Variables\n",
    "Suppose for each data, we have n-dimensional vector X and label Y. We assume that there is a linear relationship between X and Y. (ie. a.X+b=Y) thus we want to find the best condidate for (a,b). But the problem is that there are some unknown factors affecting the Y. We call them noise. <br>\n",
    "There are different ways to find (a,b). In one way, we can think about (a,b) as random variables and we want to know which setting of (a,b) is the most likely one.<br>\n",
    "We also have to consider the noise as a random variable thus we have to assign a distribution for our noise. We can imagine there are k independent unknown factors with unkown distribution affecting culmunatively the Y. Since we do not have any information about each factor, we can consider them with the following distribution:<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$PDF_k(x, \\lambda) =\n",
    "  \\begin{cases}\n",
    "    \\frac{1}{2} \\lambda  e^{- \\lambda x}       & \\quad x \\geq 0 \\\\\n",
    "    \\frac{1}{2} \\lambda  e^{ \\lambda x} & \\quad x < 0\n",
    "  \\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This PDF is in fact sum of two exponential distibutions both in negative and positve axis with the same probability. The reason why we introduce a distribution like this is that we want to emphesize there exist **at least one** simple distribution which is able to have any value with positive probability and also this distribution has one other property: <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As all we know exponential distribution is memoryless. This property helps us to be more uncertain about k different factors. <br>\n",
    "Suppose somebody told us that in the mth sample, the jth factor effect is more than r. By knowing this fact, we can not guess that how much it is bigger than r. Because we can't conclude any thing by this information and all k factors remain independent of each other.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But aside from my guess which is not proved, if we just assume there is any distribution (which has $\\mu=0$ and $\\sigma^2$) for k independent factors, and if we assume that k is large enough, central limit theorem states that the sum of all of these k factors is a Normal($\\mu=0$,$k  \\sigma^2=\\sigma '$). So "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this setting, we define Y as a random variable:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y(x, \\sigma_x) = a.X + Normal (0, \\sigma_x^2) = Normal (a.X, \\sigma_x^2 ) \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all of our factors including n factors in X and k factors which is stores in noise are independent of each other, the variance of the noise is the same for any possible X. So we can rewrite the distribution : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y(x|a,b,\\sigma) = a.X + b + Normal (0, \\sigma^2) = Normal (a.X+b, \\sigma^2 ) \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation\n",
    "Maximum likelihood estimation is a method used for finding most likely parameters setting that can generate samples data. If we have m samples $(X_i, Y_i)$, and parameter $a$ then, we can to calculate the probability of observation of data: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ P (X,Y|a,b, \\sigma) = \\displaystyle\\prod_{i=1}^{m} P(Y_i , X_i) = \\displaystyle\\prod_{i=1}^{m} P(X_i) P(Y_i | X_i) =\n",
    "\\displaystyle\\prod_{i=1}^{m} P(X_i) \\displaystyle\\prod_{i=1}^{m} P(Y_i | X_i) =\n",
    "\\displaystyle\\prod_{i=1}^{m} P(X_i) \\displaystyle\\prod_{i=1}^{m} Y(X_i|a,b,\\sigma) = L1 * \\displaystyle\\prod_{i=1}^{m} Y(X_i|a,b,\\sigma)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find $ (a,b,\\sigma)$ that maximize the above probability. Since L1 is is not a function of $ (a,b,\\sigma)$  our parameteres, we ommit it and maximize the remaining part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ P (X,Y|a,b, \\sigma) \\propto  \\displaystyle\\prod_{i=1}^{m} Y(X_i|a,b,\\sigma)=\n",
    "\\displaystyle\\prod_{i=1}^{m} Normal (a.X_i+b, \\sigma^2 ) =\n",
    "\\displaystyle\\prod_{i=1}^{m} \\frac {1}{\\sqrt{2\\pi\\sigma}} \\exp(- \\frac{(x_i-\\mu)^2}{2 \\sigma^2}) =\n",
    "(\\frac {1}{\\sqrt{2\\pi\\sigma}})^n \\exp (\\displaystyle\\sum_{i=1}^{m} (- \\frac{(x_i-\\mu)^2}{2 \\sigma^2}) =\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logarithm Function is increasing in ${\\rm I\\!R^+}$ we can maximize $log( P (X,Y|a,b, \\sigma))$ instead. (It is easier for calculating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\log (P (X,Y|a,b, \\sigma)) = -n \\log (\\sigma) +   \\displaystyle\\sum_{i=1}^{m} (- \\frac{(x_i-\\mu)^2}{2 \\sigma^2}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
