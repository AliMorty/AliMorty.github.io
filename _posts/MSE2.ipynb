{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Mean Squared Error? \n",
    "For me, a question arises when people use **MSE** as a objective function for their learning tasks. The question is: **WHY??** But if you ask this question you probably get answers such as: \n",
    "1. Since it works well on this dataset!\n",
    "2. Because we want to give more penalty for bad predictions (in comparision with l1-norm)\n",
    "3. Computating the derivation of MSE is simple (in comparision with l1-norm) <br>\n",
    "\n",
    "These reasons do not convince me. For the first reason we can say: \"ok! It works well but maybe there are some better choices.\" For the second and third reasons we can say: \"there is other alternative for giving penalty for bad predictions (such as l-4) with easy computations.\"<br> <br>\n",
    "But fortunately I found one reason (for particular situation) in Wasserman's \"All of statistics\" that makes me a little calmer! For other students not interested in the above reasons, I want to share this post to hopefully help them to be calmer. <br>\n",
    "The book states that in linear regression with normal noise if we want to use Maximum Likelihood to learn parameter, it is the same as minimizing the MSE. <br>\n",
    "First, let's define linear regression.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of Random Variables\n",
    "Suppose for each data, we have n-dimensional vector X and label Y. We assume that there is a linear relationship between X and Y. (ie. a.X+b=Y) thus we want to find the best condidate for (a,b). But the problem is that there are some unknown factors affecting the Y. We call them noise. <br>\n",
    "There are different ways to find (a,b). In one way, we can think about (a,b) as random variables and we want to know which setting of (a,b) is the most likely one.<br>\n",
    "We also have to consider the noise as a random variable so we have to assign a distribution for the noise. We can imagine there are k independent unknown factors with different unkown distribution affecting culmunatively the Y. Because there are a lot of different small factors (k > 30), there is a more general theorem than Central Limit Theorem which states that:\n",
    "> No matter if series of factor has the same distribution or not, (when our factors are small enough and have some specific properties) the distribution of their sum converges in distribution to **Normal distribution**. <br> <br> \n",
    "\n",
    "So our noise has the distribution like this:  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ PDF_{noise}(x) = Normal(x,   \\mu , \\sigma )\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can write the equation like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y(x, \\sigma_x) = a.X + Normal (0, \\sigma_x^2) = Normal (a.X, \\sigma_x^2 ) \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This PDF is in fact sum of two exponential distibutions both in negative and positve axis with the same probability. The reason why we introduce a distribution like this is that we want to emphesize there exist **at least one** simple distribution which is able to have any value with positive probability and also this distribution has one other property: <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As all we know exponential distribution is memoryless. This property helps us to be more uncertain about k different factors. <br>\n",
    "Suppose somebody told us that in the mth sample, the jth factor effect is more than r. By knowing this fact, we can not guess that how much it is bigger than r. Because we can't conclude any thing by this information and all k factors remain independent of each other.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But aside from my guess which is not proved, if we just assume there is any distribution (which has $\\mu=0$ and $\\sigma^2$) for k independent factors, and if we assume that k is large enough, central limit theorem states that the sum of all of these k factors is a Normal($\\mu=0$,$k  \\sigma^2=\\sigma '$). So "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this setting, we define Y as a random variable:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y(x, \\sigma_x) = a.X + Normal (0, \\sigma_x^2) = Normal (a.X, \\sigma_x^2 ) \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all of our factors including n factors in X and k factors which is stores in noise are independent of each other, the variance of the noise is the same for any possible X. So we can rewrite the distribution : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y(x|a,b,\\sigma) = a.X + b + Normal (0, \\sigma^2) = Normal (a.X+b, \\sigma^2 ) \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation\n",
    "Maximum likelihood estimation is a method used for finding most likely parameters setting that can generate samples data. If we have m samples $(X_i, Y_i)$, and parameter $a$ then, we can to calculate the probability of observation of data: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ P (X,Y|a,b, \\sigma) = \\displaystyle\\prod_{i=1}^{m} P(Y_i , X_i) = \\displaystyle\\prod_{i=1}^{m} P(X_i) P(Y_i | X_i) =\n",
    "\\displaystyle\\prod_{i=1}^{m} P(X_i) \\displaystyle\\prod_{i=1}^{m} P(Y_i | X_i) =\n",
    "\\displaystyle\\prod_{i=1}^{m} P(X_i) \\displaystyle\\prod_{i=1}^{m} Y(X_i|a,b,\\sigma) = L1 * \\displaystyle\\prod_{i=1}^{m} Y(X_i|a,b,\\sigma)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find $ (a,b,\\sigma)$ that maximize the above probability. Since L1 is is not a function of $ (a,b,\\sigma)$  our parameteres, we ommit it and maximize the remaining part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ P (X,Y|a,b, \\sigma) \\propto  \\displaystyle\\prod_{i=1}^{m} Y(X_i|a,b,\\sigma)=\n",
    "\\displaystyle\\prod_{i=1}^{m} Normal (a.X_i+b, \\sigma^2 ) =\n",
    "\\displaystyle\\prod_{i=1}^{m} \\frac {1}{\\sqrt{2\\pi\\sigma}} \\exp(- \\frac{(x_i-\\mu)^2}{2 \\sigma^2}) =\n",
    "(\\frac {1}{\\sqrt{2\\pi\\sigma}})^n \\exp (\\displaystyle\\sum_{i=1}^{m} (- \\frac{(x_i-\\mu)^2}{2 \\sigma^2}) =\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logarithm Function is increasing in ${\\rm I\\!R^+}$ we can maximize $log( P (X,Y|a,b, \\sigma))$ instead. (It is easier for calculating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\log (P (X,Y|a,b, \\sigma)) = -n \\log (\\sigma) +   \\displaystyle\\sum_{i=1}^{m} (- \\frac{(x_i-\\mu)^2}{2 \\sigma^2}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
